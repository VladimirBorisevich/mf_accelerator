{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MegaFon Accelerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task \n",
    "To build a model that predicts which of the three segments (0,1,2) each person belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The contest_train.csv training sample consists of the following columns:  \n",
    "\n",
    "    * ID - person's id\n",
    "    * TARGET - segment corresponding to the person.\n",
    "    * FEATURE_0…FEATURE_259 — person's characteristics.\n",
    "   \n",
    "The test sample contest_test.csv consists of an ID column followed by FEATURE_0 ... FEATURE_259.  \n",
    "The prediction accuracy is assessed using the macro-f1_score metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools \n",
    "The following libraries were used to build the model:\n",
    "* numpy - A python library for working with arrays.\n",
    "* pandas - High-level data representation and manipulation tool.\n",
    "* matplotlib - A library for creating visualizations.\n",
    "* seaborn - A library for making statistical graphics in Python.\n",
    "* scipy -  A python library for scientific and mathematical purposes.\n",
    "* statsmodels - A library for the estimation of many different statistical models.\n",
    "* sklearn - A library to solve machine learning problems.\n",
    "* imblearn - Imbalanced-learn toolbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "import seaborn as sns\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE,RandomOverSampler\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler,NearMiss\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV,cross_val_score,train_test_split\n",
    "from sklearn import feature_selection\n",
    "from sklearn.linear_model import LassoCV,SGDClassifier,RidgeClassifier,Lasso,LogisticRegressionCV,Perceptron\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score,confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler,RobustScaler,OneHotEncoder\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for writing prediction into csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_sumb(name,prediction):\n",
    "    pd.DataFrame({'ID':ID,'Predicted':prediction}).to_csv(name,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for cleaning data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_na(data):\n",
    "    '''\n",
    "    Function for cleaning data\n",
    "    Drops columns with 40 or more percentage of missing values \n",
    "    Fillining 'nan' with median in numeric and most frequent in categoric column\n",
    "    '''\n",
    "    columns_with_na = np.where(data.isna().sum()>0)[0]\n",
    "    columns_to_drop = (data.iloc[:,columns_with_na].isna().sum()/data.shape[0]*100)>40\n",
    "    columns_number = columns_with_na[np.where((data.iloc[:,columns_with_na].dtypes=='float')|(data.iloc[:,columns_with_na].dtypes=='int'))[0]]\n",
    "    columns_cat =  columns_with_na[np.where(data.iloc[:,columns_with_na].dtypes=='object')[0]]\n",
    "    data.iloc[:,columns_number] = data.iloc[:,columns_number].apply(lambda x:x.fillna(x.median()))\n",
    "    for column in columns_cat:\n",
    "        data.iloc[:,column].fillna(data.iloc[:,column].mode()[0],inplace=True)\n",
    "    data.drop(data.columns[columns_with_na[columns_to_drop]],axis=1,inplace=True)\n",
    "    data.drop('ID',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for cross validation different algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_score(model,X,y):\n",
    "    ''' \n",
    "    Function counts cross-validation score, time (overall train + prediction) of model\n",
    "    Crossval score is f1_macro, cv = 3\n",
    "    \n",
    "    '''\n",
    "    model_instance = model\n",
    "    time_before = time.time()\n",
    "    cvs = cross_val_score(model_instance,X,y,cv=3,scoring='f1_macro').mean()\n",
    "    time_after = time.time()\n",
    "    result = pd.DataFrame({'name':model.__class__.__name__,'cross_val_score':cvs,\n",
    "                           'time':time_after - time_before},index=[0])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../mf-accelerator/contest_train.csv')\n",
    "test = pd.read_csv('../mf-accelerator/contest_test.csv')\n",
    "sample = pd.read_csv('../mf-accelerator/sample_subm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    17372\n",
       "1     5650\n",
       "2     1499\n",
       "Name: TARGET, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.TARGET.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding few new binary features for missing data. If data is missing 1 esle 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4716,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_col = train.isna().sum()>0\n",
    "d = {True:1,False:0}\n",
    "na_columns = list(train.columns[na_col])\n",
    "for i in na_columns:\n",
    "    train[i+'_na'] = train.loc[:,i].isna()\n",
    "    train[i+'_na'] = train[i+'_na'].map(d,i+'_na')\n",
    "    test[i+'_na'] = test.loc[:,i].isna()\n",
    "    test[i+'_na'] = test[i+'_na'].map(d,i+'_na')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4717,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = test.ID\n",
    "cleaning_na(train)\n",
    "cleaning_na(test)\n",
    "X = train.drop(['TARGET'],axis=1)\n",
    "y = train.TARGET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale values with RobustScaler (better than StandartScaler if there are outliers).  \n",
    "Columns with small amount of unique values were probably categoric. So we can perform one hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4718,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_to_oh=[]\n",
    "for i in range(X.shape[1]):\n",
    "    if len(X.iloc[:,i].unique())<25:\n",
    "        feats_to_oh.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4719,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.iloc[:,feats_to_oh] = X.iloc[:,feats_to_oh].astype('str')\n",
    "test.iloc[:,feats_to_oh] = test.iloc[:,feats_to_oh].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4720,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = np.where((X.dtypes=='float')|(X.dtypes=='int'))[0]\n",
    "categorical_cols= np.where(X.dtypes=='object')[0]\n",
    "robust = RobustScaler()\n",
    "cat = OneHotEncoder(handle_unknown='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4721,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.iloc[:,numerical_cols] = robust.fit_transform(X.iloc[:,numerical_cols])\n",
    "test.iloc[:,numerical_cols] = robust.transform(test.iloc[:,numerical_cols])\n",
    "X_cat = cat.fit_transform(X.iloc[:,categorical_cols])\n",
    "test_cat = cat.transform(test.iloc[:,categorical_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4722,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = X.drop(X.columns[categorical_cols],axis=1,inplace=True)\n",
    "test_all = test.drop(test.columns[categorical_cols],axis=1,inplace=True)\n",
    "X_all = np.hstack((X,X_cat.toarray()))\n",
    "test_all = np.hstack((test,test_cat.toarray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "robust = RobustScaler().fit(X)\n",
    "X_robust = robust.transform(X)\n",
    "test_robust = robust.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making holdout fold. Test set should keep origin class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=4,stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some ML algorithms for comparing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers_opt = [DecisionTreeClassifier(max_depth=8,min_samples_leaf=2,random_state=4),\n",
    "                   RidgeClassifier(alpha = 1.5, max_iter = 2000, random_state=4),\n",
    "                   KNeighborsClassifier(algorithm = 'ball_tree',n_neighbors=3,weights='distance'),\n",
    "                   RandomForestClassifier(),\n",
    "                   SGDClassifier(penalty='l1',max_iter=1000,learning_rate='optimal',random_state=4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns = ['name','cross_val_score','time'])\n",
    "for classifier in classifiers_opt:\n",
    "    results = results.append(evaluate_score(classifier,X_train,y_train))    \n",
    "results.sort_values(by='cross_val_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge classifier with class_weight. Quantity of target classes differs a lot. Class_weight helps to handle class disbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5185800601109182"
      ]
     },
     "execution_count": 3414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge = RidgeClassifier(random_state=4,alpha=100,max_iter=100,class_weight={0:1,1:2.3,2:3.1})\n",
    "train = shuffle(train)\n",
    "cvs = cross_val_score(ridge,train.drop(['TARGET'],axis=1),train.TARGET,cv=5,scoring='f1_macro')\n",
    "cvs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5175,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../mf-accelerator/contest_train.csv')\n",
    "test = pd.read_csv('../mf-accelerator/contest_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_col = train.isna().sum()>0\n",
    "d = {True:1,False:0}\n",
    "na_columns = list(train.columns[na_col])\n",
    "for i in na_columns:\n",
    "    train[i+'_na'] = train.loc[:,i].isna()\n",
    "    train[i+'_na'] = train[i+'_na'].map(d,i+'_na')\n",
    "    test[i+'_na'] = test.loc[:,i].isna()\n",
    "    test[i+'_na'] = test[i+'_na'].map(d,i+'_na')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = test.ID\n",
    "cleaning_na(train)\n",
    "cleaning_na(test)\n",
    "X = train.drop(['TARGET'],axis=1)\n",
    "y = train.TARGET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual correcting class disbalance by removing majority class values. Rows are chosen randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3987,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_0 = y_train[y_train==0].index\n",
    "target_1 = y_train[y_train==1].index\n",
    "target_to_drop_0 = np.random.choice(target_0,14000,replace=False)\n",
    "target_to_drop_1 = np.random.choice(target_1,3000,replace=False)\n",
    "to_drop  = np.union1d(target_to_drop_0,target_to_drop_1)\n",
    "X_train = X_train.drop(to_drop,axis=0)\n",
    "y_train = y_train.drop(to_drop,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5243,
   "metadata": {},
   "outputs": [],
   "source": [
    "robust = RobustScaler().fit(X_train)\n",
    "X_train = robust.transform(X_train)\n",
    "X_test = robust.transform(X_test)\n",
    "test = robust.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NearMiss algorithms for undersampling dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "near = NearMiss(sampling_strategy={0:7000})\n",
    "X_res, y_res = near.fit_resample(X_train_sel, y_train)\n",
    "X_res,y_res = shuffle(X_res,y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(sampling_strategy='auto',random_state=4)\n",
    "X_res, y_res = smote.fit_resample(X_res, y_res)\n",
    "X_res,y_res = shuffle(X_res,y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    7000\n",
       "1    7000\n",
       "0    7000\n",
       "Name: TARGET, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_res.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/borisevich.vd/venv/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:525: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.866767869531941, tolerance: 0.5474142429263305\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/borisevich.vd/venv/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:525: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9600622270727399, tolerance: 0.5474142429263305\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/borisevich.vd/venv/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:525: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.1136095257579655, tolerance: 0.5474142429263305\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/borisevich.vd/venv/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:525: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 13.982859639362687, tolerance: 0.5474142429263305\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/borisevich.vd/venv/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:525: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 13.190062689128354, tolerance: 0.5474142429263305\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/borisevich.vd/venv/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:525: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 20.785297642752084, tolerance: 0.5474142429263305\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/borisevich.vd/venv/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:525: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.309886141624702, tolerance: 0.5474142429263305\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/borisevich.vd/venv/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:525: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7.892012826048813, tolerance: 0.5542169757216607\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/borisevich.vd/venv/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:525: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 41.24598866173801, tolerance: 0.5542169757216607\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/borisevich.vd/venv/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:525: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 14.877897069470237, tolerance: 0.5542169757216607\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/borisevich.vd/venv/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:525: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.5879637111484044, tolerance: 0.5542169757216607\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/borisevich.vd/venv/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:525: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9019407606429013, tolerance: 0.5542169757216607\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/borisevich.vd/venv/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:525: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 25.374592641722302, tolerance: 0.5542169757216607\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/borisevich.vd/venv/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:525: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 77.26864348793333, tolerance: 0.5542169757216607\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/borisevich.vd/venv/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:525: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.1159195074715171, tolerance: 0.5491227808577064\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/borisevich.vd/venv/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:525: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 45.23991409588416, tolerance: 0.5491227808577064\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/borisevich.vd/venv/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:525: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 57.13831769085209, tolerance: 0.5491227808577064\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/borisevich.vd/venv/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:525: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.928334773826464, tolerance: 0.5535598292232204\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/borisevich.vd/venv/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:525: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8.999577460582259, tolerance: 0.5535598292232204\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/borisevich.vd/venv/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:525: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 14.14581459476176, tolerance: 0.5535598292232204\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/borisevich.vd/venv/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:525: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.7102061679843246, tolerance: 0.5461491238131675\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/borisevich.vd/venv/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:525: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 20.556354528726843, tolerance: 0.5461491238131675\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/borisevich.vd/venv/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:525: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.09456753410268, tolerance: 0.5461491238131675\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/borisevich.vd/venv/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:525: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.7521214016887825, tolerance: 0.5461491238131675\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/borisevich.vd/venv/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:525: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 23.718567263197656, tolerance: 0.5461491238131675\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/borisevich.vd/venv/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:525: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 46.479915931719006, tolerance: 0.5461491238131675\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/borisevich.vd/venv/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:525: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 72.04832966127378, tolerance: 0.5461491238131675\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/Users/borisevich.vd/venv/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:525: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 31.0700844355124, tolerance: 0.5461491238131675\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/borisevich.vd/venv/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 676.5567262959448, tolerance: 0.6876220024469821\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "selector = feature_selection.SelectFromModel(LassoCV(random_state=4,max_iter=500)).fit(X_train,y_train)\n",
    "X_train_sel = selector.transform(X_train)\n",
    "X_test_sel = selector.transform(X_test)\n",
    "test_sel = selector.transform(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4905, 50)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_sel.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cvs: 0.5125172241320333\n",
      "f1_score:0.5270640087125986\n",
      "[[2742  608  125]\n",
      " [ 619  431   80]\n",
      " [  78   93  129]]\n"
     ]
    }
   ],
   "source": [
    "ridge = RidgeClassifier(random_state=4,alpha=1550,max_iter=100,class_weight={0:1,1:2.3,2:3.5})\n",
    "X_train,y_train = shuffle(X_train,y_train)\n",
    "cvs = cross_val_score(ridge,X_train,y_train,cv=3,scoring='f1_macro')\n",
    "ridge.fit(X_train,y_train)\n",
    "f1 = f1_score(y_test,ridge.predict(X_test),average=\"macro\")\n",
    "pred = ridge.predict(X_test)\n",
    "print('cvs:',cvs.mean())\n",
    "print(f'f1_score:{f1}')\n",
    "print(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4250,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = ridge.predict(test_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4252,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_sumb('ridge_2.csv',pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4759"
      ]
     },
     "execution_count": 4251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3475\n",
       "1    1130\n",
       "2     300\n",
       "Name: TARGET, dtype: int64"
      ]
     },
     "execution_count": 4253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cvs: 0.5722280299141174\n",
      "f1_score:0.43332991656047953\n",
      "[[1833 1064  578]\n",
      " [ 396  458  276]\n",
      " [  34   45  221]]\n"
     ]
    }
   ],
   "source": [
    "ridge = RidgeClassifier(random_state=4,alpha=10,max_iter=100,class_weight={0:1,1:1,2:1})\n",
    "X_res,y_res = shuffle(X_res,y_res)\n",
    "cvs = cross_val_score(ridge,X_res,y_res,cv=3,scoring='f1_macro')\n",
    "ridge.fit(X_res,y_res)\n",
    "f1 = f1_score(y_test,ridge.predict(X_test_sel),average=\"macro\")\n",
    "pred = ridge.predict(X_test_sel)\n",
    "print('cvs:',cvs.mean())\n",
    "print(f'f1_score:{f1}')\n",
    "print(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimal BalancedRandomForestClassifier hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_params = {\n",
    "    'sampling_strategy':['all'],\n",
    "    'criterion':['entropy'],\n",
    "    'max_depth':[None],\n",
    "    'min_samples_split':[2],\n",
    "    'min_samples_leaf':[2],\n",
    "    'min_weight_fraction_leaf':[0],\n",
    "    'max_features':[\"auto\"],\n",
    "    'max_leaf_nodes':[None],\n",
    "    'min_impurity_decrease':[0],\n",
    "    'bootstrap':[False],\n",
    "    'replacement':[True],\n",
    "    'ccp_alpha':[0]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.1 s, sys: 164 ms, total: 6.26 s\n",
      "Wall time: 2.94 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#X_train,y_train = shuffle(X_train,y_train,random_state=4)\n",
    "forest = BalancedRandomForestClassifier(n_estimators=100,random_state=4,bootstrap=False,replacement=True,\n",
    "                                        n_jobs=4,\n",
    "                                        criterion='entropy')#,class_weight={0:1,1:0.9,2:0.4})\n",
    "forest.fit(X_train_sel,y_train)\n",
    "pred = forest.predict(X_test_sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44162617909972174"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test,pred,average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "confusion_matrix shows where model faults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2078,  740,  657],\n",
       "       [ 483,  382,  265],\n",
       "       [  33,   45,  222]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5303,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = forest.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5108"
      ]
     },
     "execution_count": 5304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5305,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_sumb('balanced_forest.csv',pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#robust data 0.2 split\n",
    "#0:7000\n",
    "#n_estimators=5000 0.5293229948836577 (sum 4994)\n",
    "#n_estimators=100 0.5246443436331151\n",
    "\n",
    "#n_estimators=100  0.5279228567440777   0:1,1:1.1,2:1.5\n",
    "#n_estimators=100  0.5287763724236936   0:1,1:0.9,2:1.5\n",
    "#n_estimators=100  0.5314890514797946   0:1,1:0.9,2:1.55\n",
    "#n_estimators=5000  0.5309557718088711   0:1,1:0.9,2:1.55\n",
    "#0.5332703560478363??????\n",
    "#not robust\n",
    "#n_estimators=100  0.5282454790166882  0:1,1:0.9,2:1.55 \n",
    "#n_estimators=100  0.529150988742459  0:1,1:0.9,2:1.5 \n",
    "# not robust selected 0.005\n",
    "# 100 0.5303\n",
    "# 500 0.5306\n",
    "# 1500 0.5337932867932449\n",
    "# 3500 0.5328634909230983\n",
    "# 5000 0.5327493728153337\n",
    "# 2000 0.532857614263758\n",
    "# 1000 0.5340548236816324"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
